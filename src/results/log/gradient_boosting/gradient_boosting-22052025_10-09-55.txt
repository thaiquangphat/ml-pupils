2025-05-22 10:09:55,728 - Training arguments: {'n_folds': 5, 'n_estimators': 10, 'objective': 'multi:softmax', 'num_class': 4, 'early_stopping_rounds': 5, 'eval_metric': 'mlogloss', 'n_jobs': -1, 'tree_method': 'hist'}
2025-05-22 10:09:55,731 - Using Hyperopt to search for best hyperparameters...
2025-05-22 11:00:50,486 - Best parameters from Hyperopt: {'colsample_bytree': np.float64(0.6517241235146054), 'gamma': np.float64(3.595444436378374), 'learning_rate': np.float64(0.09053629164994359), 'max_depth': np.float64(9.0), 'min_child_weight': np.float64(2.0), 'reg_alpha': np.float64(16.861141568695626), 'reg_lambda': np.float64(0.2698356414378619), 'subsample': np.float64(0.5477893965196412)}
2025-05-22 11:00:50,508 - Best hyperparameters saved at results/models/gradient_boosting/best_hyperparams_22052025_11-00-50.json
2025-05-22 11:00:50,509 - Retraining best model on full dataset with early stopping to find best n_estimators...
2025-05-22 11:14:45,607 - Best iteration determined via early stopping: 9
2025-05-22 11:14:45,626 - Training final model on full data with optimal hyperparameters and n_estimators...
2025-05-22 11:31:20,401 - Model saved at results/models/gradient_boosting/xgboost_22052025_11-31-20.pkl
